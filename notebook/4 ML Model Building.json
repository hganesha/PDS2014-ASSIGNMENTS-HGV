{
	"name": "4 ML Model Building",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"language_info": {
				"name": "python"
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"{\n",
					"  \"metadata\": {\n",
					"    \"saveOutput\": true,\n",
					"    \"language_info\": {\n",
					"      \"name\": \"python\"\n",
					"    }\n",
					"  },\n",
					"  \"nbformat\": 4,\n",
					"  \"nbformat_minor\": 2,\n",
					"  \"cells\": [\n",
					"    {\n",
					"      \"cell_type\": \"markdown\",\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"Copyright (c) Microsoft Corporation. \\n\",\n",
					"        \"Licensed under the MIT license. \\n\",\n",
					"        \"# ML Model Building\\n\",\n",
					"        \"\\n\",\n",
					"        \"Pre-process data and use the data to build a Spark machine learning model in this notebook using the following steps:\\n\",\n",
					"        \"\\n\",\n",
					"        \"1. Training-test split\\n\",\n",
					"        \"1. Data pre-processing (one-hot encoding, vectorizor)\\n\",\n",
					"        \"1. Build machine learning model\\n\",\n",
					"        \"1. Calculate model performance metrics\\n\",\n",
					"        \"1. Extract model feature importances\\n\",\n",
					"        \"1. Save results to data lake\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"markdown\",\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"## Library Imports\\n\",\n",
					"        \"\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"import pyspark\\n\",\n",
					"        \"spark = pyspark.sql.SparkSession.builder.appName(\\\"MyApp\\\") \\\\\\n\",\n",
					"        \"               .config(\\\"spark.jars.packages\\\", \\\"com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1\\\") \\\\\\n\",\n",
					"        \"               .config(\\\"spark.jars.repositories\\\", \\\"https://mmlspark.azureedge.net/maven\\\") \\\\\\n\",\n",
					"        \"               .getOrCreate()\\n\",\n",
					"        \"from mmlspark.lightgbm import *\\n\",\n",
					"        \"from pyspark.sql import functions as F\\n\",\n",
					"        \"from pyspark.sql.functions import *\\n\",\n",
					"        \"from pyspark.sql import DataFrame\\n\",\n",
					"        \"from pyspark.sql.types import *\\n\",\n",
					"        \"from pyspark.ml.feature import *\\n\",\n",
					"        \"from pyspark.ml import Pipeline\\n\",\n",
					"        \"from pyspark.ml.linalg import Vectors\\n\",\n",
					"        \"from pyspark.ml.tuning import *\\n\",\n",
					"        \"from pyspark.ml.evaluation import *\\n\",\n",
					"        \"from pyspark.ml.classification import *\\n\",\n",
					"        \"import pandas as pd\\n\",\n",
					"        \"import numpy as np\\n\",\n",
					"        \"spark.conf.set('spark.sql.execution.arrow.enabled', False)\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"markdown\",\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"## Read In Data From Delta Lake\\n\",\n",
					"        \"\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"data_lake_account_name = ''\\n\",\n",
					"        \"file_system_name = ''\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"df = spark.read.format(\\\"delta\\\").load(f\\\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/transformed_data/ml_data\\\")\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"markdown\",\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"## Train-Test Split\\n\",\n",
					"        \"Split data into a 70-30 training-test split\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"(trainDF, testDF) = df.randomSplit([.7, .3], seed = 123)\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"markdown\",\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"## ML Pre-Processing & Model Building\\n\",\n",
					"        \"1. Pre-process data by encoding categorical columns and assembling them into a vector format expected for model building.\\n\",\n",
					"        \"2. Build a Spark pipeline binary classifier model to predict growth using LightGBM\\n\",\n",
					"        \"3. Use this model to score the test dataset to get model performance metrics\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"# Target column (label)\\n\",\n",
					"        \"target_col = 'growth'\\n\",\n",
					"        \"\\n\",\n",
					"        \"# ID columns\\n\",\n",
					"        \"id_col_1 = 'user_id'\\n\",\n",
					"        \"id_col_2 = 'year'\\n\",\n",
					"        \"id_col_3 = 'month'\\n\",\n",
					"        \"\\n\",\n",
					"        \"# Separate into Categorical, Target, and Numeric Columns\\n\",\n",
					"        \"\\n\",\n",
					"        \"# Create categorical column list with all of the columns that contain int and string values\\n\",\n",
					"        \"categorical_cols = ['brand_apple_purchased_binary', 'brand_samsung_purchased_binary', 'brand_xiaomi_purchased_binary', \\n\",\n",
					"        \"                    'brand_huawei_purchased_binary', 'brand_acer_purchased_binary', 'subcategory_smartphone_purchased_binary', \\n\",\n",
					"        \"                    'subcategory_audio_purchased_binary', 'subcategory_clocks_purchased_binary', \\n\",\n",
					"        \"                    'subcategory_tablet_purchased_binary', 'subcategory_telephone_purchased_binary', \\n\",\n",
					"        \"                    'product_id_1004856_purchased_binary', 'product_id_1004767_purchased_binary', \\n\",\n",
					"        \"                    'product_id_1005115_purchased_binary', 'product_id_4804056_purchased_binary', 'product_id_1004833_purchased_binary']\\n\",\n",
					"        \"\\n\",\n",
					"        \"numeric_cols = ['sessions_per_user_per_month', 'avg_session_duration_per_user_per_month', 'avg_conversion_rate_per_user_per_month',\\n\",\n",
					"        \"                'avg_order_value_per_user_per_month', 'avg_cart_abandon_rate']\\n\",\n",
					"        \"\\n\",\n",
					"        \"stages = [] # stages in our Pipeline\\n\",\n",
					"        \"\\n\",\n",
					"        \"# Category Indexing with StringIndexer - Use OneHotEncoder to convert categorical variables into binary SparseVectors\\n\",\n",
					"        \"string_indexes = [StringIndexer(inputCol = c, outputCol = 'idx_' + c, handleInvalid = 'keep') for c in categorical_cols]\\n\",\n",
					"        \"onehot_indexes = [OneHotEncoderEstimator(inputCols = ['idx_' + c], outputCols = ['ohe_' + c]) for c in categorical_cols]\\n\",\n",
					"        \"stages += string_indexes + onehot_indexes\\n\",\n",
					"        \"\\n\",\n",
					"        \"# Transform all numeric features into a vector using VectorAssembler\\n\",\n",
					"        \"assembler_inputs = ['ohe_' + c for c in categorical_cols] + numeric_cols\\n\",\n",
					"        \"assembler = VectorAssembler(inputCols = assembler_inputs, outputCol = 'features', handleInvalid = 'keep')\\n\",\n",
					"        \"stages += [assembler]\\n\",\n",
					"        \"\\n\",\n",
					"        \"# Create an indexed label from your target variable\\n\",\n",
					"        \"label_string_idx = StringIndexer(inputCol = target_col, outputCol = 'label', handleInvalid = 'keep')\\n\",\n",
					"        \"stages += [label_string_idx]\\n\",\n",
					"        \"\\n\",\n",
					"        \"# Set a random seed variable for reproducibility\\n\",\n",
					"        \"random_seed_val = 12345\\n\",\n",
					"        \"\\n\",\n",
					"        \"# Light GBM Classifier\\n\",\n",
					"        \"lgbm = LightGBMClassifier(learningRate = 0.1, numIterations = 100, numLeaves = 50)\\n\",\n",
					"        \"stages += [lgbm]\\n\",\n",
					"        \"\\n\",\n",
					"        \"lgbmPipeline = Pipeline(stages = stages)\\n\",\n",
					"        \"lgbmPipelineModel = lgbmPipeline.fit(trainDF)\\n\",\n",
					"        \"lgbmDF = lgbmPipelineModel.transform(testDF)\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"markdown\",\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"## Model Performance Metrics\\n\",\n",
					"        \"Calculate classification model metrics using the test dataset\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"mce = MulticlassClassificationEvaluator()\\n\",\n",
					"        \"bce = BinaryClassificationEvaluator()\\n\",\n",
					"        \"\\n\",\n",
					"        \"accuracy = mce.setMetricName('accuracy').evaluate(lgbmDF)\\n\",\n",
					"        \"precision = mce.setMetricName('weightedPrecision').evaluate(lgbmDF)\\n\",\n",
					"        \"recall = mce.setMetricName('weightedRecall').evaluate(lgbmDF)\\n\",\n",
					"        \"f1 = mce.setMetricName('f1').evaluate(lgbmDF)\\n\",\n",
					"        \"auc = bce.setMetricName('areaUnderROC').evaluate(lgbmDF)\\n\",\n",
					"        \"\\n\",\n",
					"        \"# model metrics df\\n\",\n",
					"        \"model_metrics = spark.createDataFrame(\\n\",\n",
					"        \"    [\\n\",\n",
					"        \"        ('Accuracy', f'{accuracy:.2f}'),\\n\",\n",
					"        \"        ('Precision', f'{precision:.2f}'),\\n\",\n",
					"        \"        ('Recall', f'{recall:.2f}'),\\n\",\n",
					"        \"        ('F1 Score', f'{f1:.2f}'),\\n\",\n",
					"        \"        ('AUC', f'{auc:.2f}'),\\n\",\n",
					"        \"    ],\\n\",\n",
					"        \"    ['Metric', 'Value']\\n\",\n",
					"        \")\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {\n",
					"        \"diagram\": {\n",
					"          \"activateDiagramType\": 1,\n",
					"          \"chartConfig\": {\n",
					"            \"category\": \"bar\",\n",
					"            \"keys\": [\n",
					"              \"Metric\"\n",
					"            ],\n",
					"            \"values\": [\n",
					"              \"Metric\"\n",
					"            ],\n",
					"            \"yLabel\": \"Metric\",\n",
					"            \"xLabel\": \"Metric\",\n",
					"            \"aggregation\": \"COUNT\",\n",
					"            \"aggByBackend\": false\n",
					"          },\n",
					"          \"aggData\": \"{\\\"Metric\\\":{\\\"AUC\\\":1,\\\"Accuracy\\\":1,\\\"F1 Score\\\":1,\\\"Precision\\\":1,\\\"Recall\\\":1}}\",\n",
					"          \"isSummary\": false,\n",
					"          \"previewData\": {\n",
					"            \"filter\": null\n",
					"          },\n",
					"          \"isSql\": false\n",
					"        }\n",
					"      },\n",
					"      \"source\": [\n",
					"        \"display(model_metrics)\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"markdown\",\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"## Feature Importances\\n\",\n",
					"        \"Use the model feature importances to determine the top revenue growth factors and their relative importances\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"# Custom function to extract feature names and importance - partly borrowed from https://gist.github.com/timlrx/1d5fdb0a43adbbe32a9336ba5c85b1b2#file-featureimportanceselector-py\\n\",\n",
					"        \"def ExtractFeatureImp(featureImp, df, featuresCol):\\n\",\n",
					"        \"    list_extract = []\\n\",\n",
					"        \"    for i in df.schema[featuresCol].metadata['ml_attr']['attrs']:\\n\",\n",
					"        \"        list_extract = list_extract + df.schema[featuresCol].metadata['ml_attr']['attrs'][i]\\n\",\n",
					"        \"    varlist = pd.DataFrame(list_extract)\\n\",\n",
					"        \"    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\\n\",\n",
					"        \"    return(varlist.sort_values('score', ascending = False))\\n\",\n",
					"        \"  \\n\",\n",
					"        \"varlist = ExtractFeatureImp(lgbmPipelineModel.stages[-1].getFeatureImportances(), lgbmDF, 'features')\\n\",\n",
					"        \"\\n\",\n",
					"        \"# important features df\\n\",\n",
					"        \"important_features = spark.createDataFrame(varlist)\\n\",\n",
					"        \"important_features = important_features.drop('idx')\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {\n",
					"        \"diagram\": {\n",
					"          \"activateDiagramType\": 1,\n",
					"          \"chartConfig\": {\n",
					"            \"category\": \"bar\",\n",
					"            \"keys\": [\n",
					"              \"name\"\n",
					"            ],\n",
					"            \"values\": [\n",
					"              \"score\"\n",
					"            ],\n",
					"            \"yLabel\": \"score\",\n",
					"            \"xLabel\": \"name\",\n",
					"            \"aggregation\": \"SUM\",\n",
					"            \"aggByBackend\": false\n",
					"          },\n",
					"          \"aggData\": \"{\\\"score\\\":{\\\"avg_cart_abandon_rate\\\":505,\\\"avg_conversion_rate_per_user_per_month\\\":937,\\\"avg_order_value_per_user_per_month\\\":1117,\\\"avg_session_duration_per_user_per_month\\\":1174,\\\"ohe_brand_acer_purchased_binary_0\\\":34,\\\"ohe_brand_acer_purchased_binary_1\\\":0,\\\"ohe_brand_apple_purchased_binary_0\\\":56,\\\"ohe_brand_apple_purchased_binary_1\\\":3,\\\"ohe_brand_huawei_purchased_binary_0\\\":25,\\\"ohe_brand_huawei_purchased_binary_1\\\":0,\\\"ohe_brand_samsung_purchased_binary_0\\\":50,\\\"ohe_brand_samsung_purchased_binary_1\\\":6,\\\"ohe_brand_xiaomi_purchased_binary_0\\\":40,\\\"ohe_brand_xiaomi_purchased_binary_1\\\":3,\\\"ohe_product_id_1004767_purchased_binary_0\\\":20,\\\"ohe_product_id_1004767_purchased_binary_1\\\":2,\\\"ohe_product_id_1004833_purchased_binary_0\\\":11,\\\"ohe_product_id_1004833_purchased_binary_1\\\":0,\\\"ohe_product_id_1004856_purchased_binary_0\\\":18,\\\"ohe_product_id_1004856_purchased_binary_1\\\":0,\\\"ohe_product_id_1005115_purchased_binary_0\\\":21,\\\"ohe_product_id_1005115_purchased_binary_1\\\":1,\\\"ohe_product_id_4804056_purchased_binary_0\\\":26,\\\"ohe_product_id_4804056_purchased_binary_1\\\":1,\\\"ohe_subcategory_audio_purchased_binary_0\\\":79,\\\"ohe_subcategory_audio_purchased_binary_1\\\":9,\\\"ohe_subcategory_clocks_purchased_binary_0\\\":55,\\\"ohe_subcategory_clocks_purchased_binary_1\\\":6,\\\"ohe_subcategory_smartphone_purchased_binary_0\\\":66,\\\"ohe_subcategory_smartphone_purchased_binary_1\\\":11,\\\"ohe_subcategory_tablet_purchased_binary_0\\\":9,\\\"ohe_subcategory_tablet_purchased_binary_1\\\":0,\\\"ohe_subcategory_telephone_purchased_binary_0\\\":3,\\\"ohe_subcategory_telephone_purchased_binary_1\\\":0,\\\"sessions_per_user_per_month\\\":612}}\",\n",
					"          \"isSummary\": false,\n",
					"          \"previewData\": {\n",
					"            \"filter\": null\n",
					"          },\n",
					"          \"isSql\": false\n",
					"        }\n",
					"      },\n",
					"      \"source\": [\n",
					"        \"display(important_features)\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"markdown\",\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"## Save Results to Data Lake\\n\",\n",
					"        \"Persist the model results to Delta tables on the Data Lake\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    },\n",
					"    {\n",
					"      \"cell_type\": \"code\",\n",
					"      \"execution_count\": null,\n",
					"      \"outputs\": [],\n",
					"      \"metadata\": {},\n",
					"      \"source\": [\n",
					"        \"important_features.write.format(\\\"delta\\\").mode(\\\"overwrite\\\").option(\\\"overwriteSchema\\\", \\\"true\\\").save(f\\\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/important_features/important_features\\\")\\n\",\n",
					"        \"model_metrics.write.format(\\\"delta\\\").mode(\\\"overwrite\\\").option(\\\"overwriteSchema\\\", \\\"true\\\").save(f\\\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/model_metrics/model_metrics\\\")\"\n",
					"      ],\n",
					"      \"attachments\": {}\n",
					"    }\n",
					"  ]\n",
					"}"
				],
				"execution_count": null
			}
		]
	}
}